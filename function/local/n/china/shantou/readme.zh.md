# 基于重复博弈的激励型教育仿真缺陷修复

## 摘要

针对一个以激励函数（平均成绩与本科升学率）驱动的多主体、时间序列教育行政仿真中的若干缺陷，本文在重复博弈框架下进行识别与修复。原实现将每一期视为一次性博弈，导致：（1）同一期内策略选择与施加存在顺序依赖；（2）策略不具备历史依赖与视界依赖；（3）在无任何学生策略组人数达到阈值时，“最佳策略”统计输出错误。我们引入重复博弈中的视界与历史、同期同时行动以及声誉机制进行修正，并将修改要点整理成本文。

**关键词：** 重复博弈；激励设计；教育行政；多主体仿真；时间序列模型。

---

## 1. 引言

本仿真在给定时间窗口内对一所学校中的班级进行建模：教师（Y、F）、命名与随机学生、心理老师与学校领导。每日各在册主体按角色选择策略（如正常教学、PUA 减员、减少高考参考人数、努力学习、回避对抗、休学退学、网络暴力等），后果更新成绩、压力、参考池与退学状态；领导层“政绩”由时间激励函数对平均成绩与本科升学率的加权给出。原代码存在三方面问题：

1. **同期顺序依赖：** 策略选择与后果施加在同一轮对主体的遍历中完成，先被处理的主体的后果会立刻改变状态，后处理的主体在同一期内看到的是已被修改的状态，导致结果依赖主体遍历顺序，与“同期同时行动”的阶段博弈解释不一致。
2. **一次性阶段博弈：** 策略仅依赖当前聚合状态，未利用过去行动（如“上期教师是否背叛”）与剩余期数（如终局效应），与重复博弈中的视界与历史依赖不符。
3. **统计输出错误：** “按平均成绩/按留校率的最佳策略”仅在存在人数≥3 的策略组时更新；当所有策略组人数均&lt;3 时，最佳成绩与最佳留校率保持为 −1，导致输出无效。

我们通过以下方式修复：（1）将每期拆为**两阶段**——先全体在相同上下文与历史下选策略，再统一施加后果，消除同期顺序依赖；（2）增加**重复博弈状态**（剩余步数、上期教师背叛、每人上期策略），并使策略选择**依赖历史与视界**；（3）在无任何组人数≥3 时，对**最佳策略**做**回退**：在所有非空组中按平均成绩与留校率取最优，保证输出始终有效。

---

## 2. 重复博弈框架

### 2.1 阶段博弈与重复

- **阶段博弈：** 每一天为一期；在册主体为参与者，每人从有限策略集中选一个行动；收益隐含在后果（成绩、压力、参考池、退学）及领导的激励函数中。
- **重复博弈：** 阶段博弈在有限期数内重复（从仿真开始到结束）。假定参与者可依赖（i）**剩余视界**（剩余期数）与（ii）**上期教师“背叛”**（使用 PUA 或减少参考人数）。

### 2.2 合作与背叛（操作化）

- **教师：** “合作”=正常教学；“背叛”=教师 Y 的 PUA、教师 F 的减少高考参考人数。
- **学生：** “合作”=努力学习或回避对抗；“背叛”=如犹大的网络暴力、P/普通生的休学等。此处不形式化支付，仅使用**声誉**与**视界**：教师背叛后学生可“惩罚”（如回避、犹大升级）；教师可在下一期“恢复声誉”而合作。

### 2.3 重复博弈状态的实现

- **SimContext** 扩展为包含：
  - `StepsRemaining`：剩余期数（含当期），供策略依赖视界（如终局）。
  - `LastRoundTeacherDefection`：上一期是否有教师采用 PUA 或减少参考人数。
- **Agent** 扩展为包含：
  - `LastStrategy`：上一期所选策略（可供后续历史依赖使用；当前策略规则主要使用 `LastRoundTeacherDefection` 与 `StepsRemaining`）。
- **Run 循环：** 每期开始时设置 `ctx.StepsRemaining = steps - i`、`ctx.LastRoundTeacherDefection = lastRoundTeacherDefection`；每期施加完所有后果后，根据本期的教师行动更新 `lastRoundTeacherDefection`。

---

## 3. 两阶段步进（同期同时行动）

原先在单次遍历中：选策略 → 施加后果（可能改变其他主体），导致同一期内后处理的主体看到先处理者造成的状态变化。

**修改：** 每期分为两阶段。

1. **阶段一——选择：** 对每个在册主体调用 `ChooseStrategy(now, agent, &ctx)`，将所得策略存入按主体索引的数组；本阶段不修改任何状态，所有人看到同一 `ctx` 与同一历史。
2. **阶段二——施加：** 对每个主体按已存策略施加后果（更新成绩、压力、参考池、退学、事件），并写入 `agent.LastStrategy`；若本期内有教师使用 PUA 或减少参考人数，则置 `lastRoundTeacherDefection = true` 供下一期使用。

从而消除同期顺序依赖，使阶段博弈在同期上符合“同时行动”的设定。

---

## 4. 历史与视界依赖的策略规则

### 4.1 教师

- **教师 Y（以平均分为激励）：**  
  - 若法规/道德风险高 → 撒谎躲避。  
  - **重复博弈：** 若 `LastRoundTeacherDefection` 且 `StepsRemaining > 1` → 正常教学（恢复声誉）。  
  - 否则若平均分低且学生人数多 → PUA。

- **教师 F（成绩+升学率）：**  
  - 若法规/道德风险高 → 撒谎躲避。  
  - **重复博弈：** 若 `LastRoundTeacherDefection` 且 `StepsRemaining > 1` → 正常教学。  
  - 否则若参考人数多且升学率低 → 减少参考人数。

即教师在“背叛”后、若还有未来期数，会至少合作一期；条件再次满足时可再次背叛。

### 4.2 学生

- **犹大：** 在学生人数多且自身法规风险低时：若 `StepsRemaining <= 1`（终局）或 `LastRoundTeacherDefection`（报复）→ 网络暴力；否则努力学习。
- **普通学生：** 净 PUA 与压力高 → 休学。**重复博弈：** 若 `LastRoundTeacherDefection` 且 `StepsRemaining > 2` → 回避对抗（降低暴露）。否则按 IQ：高则努力学习，否则回避。

其他学生角色（P、Y、C13）及心理老师、领导的规则未改，仅置于两阶段、同一上下文的框架下运行。

---

## 5. 统计输出缺陷修复（无组人数≥3 时的最佳策略）

原先“按平均成绩最佳策略”与“按留校率最佳策略”仅在存在人数≥3 的组时更新；当所有策略组人数均&lt;3 时，`bestScore`、`bestStayRate` 保持为 −1，打印出的“最佳”数值无效。

**修复：** 在按策略组统计的主循环之后：

- 若 `bestScore < 0`，对所有非空组做一次回退循环，按平均成绩取最高者作为最佳策略（及数值）。
- 若 `bestStayRate < 0`，对所有非空组做一次回退循环，按留校率取最高者作为最佳策略（及数值）。

从而在样本不足时仍输出有意义的“最佳”策略与数值，在有≥3 人组时优先使用该组统计。

---

## 6. 代码与行为变更小结

| 项目 | 变更内容 |
|------|----------|
| **SimContext** | 增加 `StepsRemaining`、`LastRoundTeacherDefection`。 |
| **Agent** | 增加 `LastStrategy`。 |
| **Run** | 每期两阶段：先选齐策略，再统一施加并更新上期背叛标志。 |
| **教师 Y / F** | 背叛后且剩余期数&gt;1 时合作一期；其余保持原条件触发背叛。 |
| **犹大** | 在终局或教师上期背叛且条件满足时使用网络暴力。 |
| **普通学生** | 教师上期背叛且剩余期数足够时优先回避对抗。 |
| **printStudentBestStrategy** | 当无组人数≥3 时，在所有非空组中回退取最佳。 |

---

## 7. 结论

在激励驱动的多主体、离散时间仿真中引入重复博弈的同期同时行动、视界与一期“背叛”历史，修复了同期顺序依赖、策略无历史/视界依赖以及最佳策略统计在样本不足时的错误。修改保持原有时间序列与激励函数结构，通过最少的状态与流程扩展，使行为符合重复博弈下的声誉与终局逻辑，并保证输出统计的稳健性。

---

## 参考文献

1. Fudenberg, D., & Tirole, J. (1991). *Game Theory*. MIT Press.  
2. Mailath, G. J., & Samuelson, L. (2006). *Repeated Games and Reputations: Long-Run Relationships*. Oxford University Press.  
3. 本包设计说明：`y.md`（时间序列模型、角色、策略与激励函数）。
